{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell to import everything we'll need.\n",
        "from importlib import reload\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import unittest\n",
        "\n",
        "# Custom code for this assignment\n",
        "import graph; reload(graph)\n",
        "import graph_test; reload(graph_test)\n",
        "\n",
        "# You may get a warning about runtime versions when running this cell.\n",
        "# Don't worry about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Note:** If you get an error like\n",
        "\n",
        "```\n",
        ".../importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
        "```\n",
        "\n",
        "You can safely ignore it - everything should still work fine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fun with TensorFlow\n",
        "\n",
        "The goal of this section is to familiarize yourself with the Python [TensorFlow API](https://www.tensorflow.org/api_docs/python/index.html). We'll be using TensorFlow throughout the class to implement deep learning models, which are the state-of-the-art on many NLP tasks such as machine translation, sentiment analysis, and language modeling.\n",
        "\n",
        "### TensorFlow: Declarative Numerical Programming\n",
        "\n",
        "The TensorFlow programming model has two phases:\n",
        "1.  **Construct a graph** by running Python code\n",
        "2.  **Execute the graph** by calling `session.run()`\n",
        "\n",
        "In the **graph construction** phase, we operate on everything symbolically. Executing the Python code doesn't actually do any numerical calculations - it just tells TensorFlow how to do the computation later. Every variable you define here is a **Tensor**, which creates a node in the computation graph.\n",
        "\n",
        "In the **execution phase**, we give TensorFlow input data and a list of output operations. It runs the data through the graph and returns numerical results as NumPy arrays.\n",
        "\n",
        "#### Tensor Objects\n",
        "\n",
        "Tensor objects are the symbolic equivalent of NumPy arrays, and support many similar operations. For example, to compute a linear model $y = vW + b$ in NumPy, you might do:\n",
        "```python\n",
        "# w, v are np.ndarray\n",
        "y = np.dot(v, w) + b\n",
        "```\n",
        "In TensorFlow, this would be expressed as:\n",
        "```python\n",
        "# w, v, b are tf.Tensor\n",
        "y = tf.matmul(v, w) + b\n",
        "```\n",
        "\n",
        "There are a few ways to define Tensors, but the most important are:\n",
        "\n",
        "- **[Constants and sequences](https://www.tensorflow.org/versions/r0.10/api_docs/python/constant_op.html#constants-sequences-and-random-values)**, like tf.constant(), tf.zeros(), or tf.linspace(). These create a Tensor with a fixed value, and pretty much work like their NumPy equivalents.\n",
        "\n",
        "- **[Variables](https://www.tensorflow.org/versions/r0.10/how_tos/variables/index.html)**, which are persistent and can be modified during execution. Think model parameters, which get updated by training.\n",
        "\n",
        "- **[Placeholders](https://www.tensorflow.org/versions/r0.10/api_docs/python/io_ops.html#placeholder)**, which are used for data inputs. You feed these in by passing a NumPy array at execution time.\n",
        "\n",
        "Operations on tensors - like `tf.matmul()` or `tf.nn.softmax()` - produce other tensors and add additional nodes to the graph.\n",
        "\n",
        "#### Delayed Execution\n",
        "\n",
        "The key difference between the NumPy code `y = np.dot(v, w) + b` and the TensorFlow equivalent `y = tf.matmul(v, w) + b` is that when the latter runs, it _doesn't actually do the computation_. Instead, it tells TensorFlow that `y` is derived by performing the `matmul` operation on `v` and `w`, followed by adding `b`. We refer to this as \"building the graph.\" In order to crunch the numbers, you need to run the graph, such as:\n",
        "```python\n",
        "# w, b defined as persistent tf.Variable, assume w is 10-dimensional vector\n",
        "y = tf.matmul(v, w) + b  # Add Op (Tensor) to the graph\n",
        "y_value = session.run(y, feed_dict={v=np.ones(10)})  # Run the graph\n",
        "```\n",
        "where `feed_dict` is how we \"feed\" input (NumPy arrays) to TensorFlow, and `y_value` will be a NumPy array containing the result of the computation.\n",
        "\n",
        "This seems clunky for such a simple example - but it will dramatically simplify things when we start working with more complicated models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Adder\n",
        "\n",
        "Below is the code to construct a simple adder in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = 5.0\n",
        "y = 7.0\n",
        "np.add(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As noted about delayed execution, the equivalent TensorFlow consists of two parts:\n",
        "1.  Construct a computational graph.\n",
        "2.  Execute it.\n",
        "\n",
        "The graph will look something like this:\n",
        "<img src=\"addgraph.png\" alt=\"Graph\" style=\"width: 200px;\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z = tf.add(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z], feed_dict={x: 5.0, y: 7.0}))\n",
        "\n",
        "# Note, you can re-execute the same computational graph with different inputs:\n",
        "print(sess.run([z], feed_dict={x: 1.0, y: 2.0}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The python API adds \"syntactic sugar\" whenever you do \"normal\" python binary operations on tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z = x + y  # Syntactic sugar for tf.add(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z], feed_dict={x: 5.0, y: 7.0}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can construct more complex graphs, and selectively execute them.  For example, here's a graph that contains both addition and subtraction.\n",
        "\n",
        "<img src=\"addsubgraph.png\" alt=\"Addition and Subtraction in one graph\" style=\"width: 200px;\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z_add = tf.add(x, y)\n",
        "z_sub = tf.subtract(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "feed_dict = {x: 5.0, y: 7.0}\n",
        "print(\"Adding: \", sess.run([z_add], feed_dict=feed_dict))\n",
        "print(\"Subtracting: \", sess.run([z_sub], feed_dict=feed_dict))\n",
        "print(\"Both: \", sess.run([z_add, z_sub], feed_dict=feed_dict))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A 1. Coding Exercise\n",
        "\n",
        "In the cell below, construct a computational graph that accepts three inputs and computes both:\n",
        "1. `x1 * x2`\n",
        "2. `(x1 * x2) + x3`\n",
        "\n",
        "Use as few nodes in the graph as possible (i.e. use the fact the 1 is a sub-graph of 2).\n",
        "\n",
        "_Hint:_ Use the \"Both\" approach from the previous cell.  For performance reasons, when called this way (multiple outputs at once), TensorFlow is \"smart\" enough to compute `x1 * x2` only once and use it in both 1 and 2.\n",
        "\n",
        "Evaluate them for each of the following:\n",
        "\n",
        "* `{x1: 2.0, x2: 3.0, x3: 4.0}`\n",
        "* `{x1: 5.0, x2: 3.0, x3: 4.0}`\n",
        "* `{x1: 6.0, x2: 3.0, x3: 4.0}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Execute it.\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you did the exercise correctly, you should see:\n",
        "```\n",
        "[6.0, 10.0]\n",
        "[15.0, 19.0]\n",
        "[18.0, 22.0]\n",
        "```\n",
        "TensorFlow can perform operations on more than just scalars - we'll commonly use it to manipulate vectors, matricies,  vectors, matrices, and occasionally higher-order tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z_add = tf.add(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z_add], feed_dict={x: [5.0, 4.0], y: [7.0, 1.0]}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can pass TensorFlow matrices from NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z_add = tf.add(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z_add], feed_dict={x: np.ones([3, 3]), \n",
        "                                   y: 3*np.ones([3, 3])}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can construct parts of graphs inside functions and return references to various operations in the graph.  This is a convenient way to construct the same subgraph in multiple places."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "def DoComplexComputation(a, b):\n",
        "    \"\"\"This function adds operations to the current TensorFlow graph.\n",
        "    \n",
        "    Args:\n",
        "      a: Tensor (scalar)\n",
        "      b: Tensor (scalar)\n",
        "      \n",
        "    Returns:\n",
        "      Tensor (scalar)\n",
        "    \"\"\"\n",
        "    return tf.add(tf.multiply(a, a), b) + tf.constant(3.6)\n",
        "\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "\n",
        "z_one_application = DoComplexComputation(x, y)\n",
        "z_two_applications = DoComplexComputation(z_one_application, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z_one_application, z_two_applications], feed_dict={x: 7.0, y: 9.0}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TensorFlow will broadcast shapes, similar to NumPy.  As you can see below, it does this silently and with no explicit opt-in from the user. If you're not careful, this can lead to subtle bugs!\n",
        "\n",
        "For more information on how broadcasting works, see [NumPy broadcasting](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html) and [Broadcasting: the good and the ugly](https://github.com/vahidk/EffectiveTensorflow#broadcasting-the-good-and-the-ugly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct the computational graph.\n",
        "tf.reset_default_graph()\n",
        "x = tf.placeholder(tf.float32)\n",
        "y = tf.placeholder(tf.float32)\n",
        "z_add = tf.add(x, y)\n",
        "\n",
        "# Execute it.\n",
        "sess = tf.Session()\n",
        "print(sess.run([z_add], feed_dict={x: [7.0, 3.0, 4.0], y: 2.0}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# A 2. Play with More APIs\n",
        "\n",
        "Implement the missing parts of the subsequent code fragments.\n",
        "\n",
        "Hint: see the [TensorFlow Documentation](https://www.tensorflow.org/api_docs/python/tf/) for the APIs in question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tf.reset_default_graph()\n",
        "W = np.array([[1, 2], [4, 5], [7, 8], [9, 10]])\n",
        "# Uncomment to help debug:  print('W:\\n', W)\n",
        "x = np.array([[8, 8, 8, 8], [7, 7, 7, 7]])\n",
        "# Uncomment to help debug:  print('x:\\n', x)\n",
        "b = np.array([4, 3])\n",
        "# Uncomment to help debug:  print('b:\\n', b)\n",
        "\n",
        "numpy_value = x.dot(W) + b\n",
        "# Uncomment to help debug:  print('np result:\\n', numpy_value)\n",
        "\n",
        "# Construct placeholders for all the inputs.\n",
        "W_ph = tf.placeholder(tf.float32)\n",
        "x_ph = tf.placeholder(tf.float32)\n",
        "b_ph = tf.placeholder(tf.float32)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# Build an affine transform calculation, xW + b, using tf.matmul and tf.add below.\n",
        "# Be sure to understand the broadcasting discussion above.\n",
        "# affine_matmul_add = \n",
        "\n",
        "# This affine\n",
        "# Create another part of the graph that computes it with tf.nn.xw_plus_b.\n",
        "# affine_xw_plus_b =\n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "# Execute the graph.\n",
        "sess = tf.Session()\n",
        "affine_matmul_add_val, affine_xw_plus_b_val = sess.run(\n",
        "    [affine_matmul_add, affine_xw_plus_b],  # Desired nodes to evaluate.\n",
        "    feed_dict={W_ph: W, x_ph: x, b_ph: b})  # Desired input.\n",
        "\n",
        "assert np.all(numpy_value == affine_matmul_add_val)\n",
        "assert np.all(numpy_value == affine_xw_plus_b_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# Use tf.nn.embedding_lookup to extract:\n",
        "# a) the first row of W.\n",
        "# b) the second and third rows of W.\n",
        "\n",
        "W_ph = tf.placeholder(tf.float32)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# first_row =\n",
        "\n",
        "# second_and_third_rows = \n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "W = np.array([[1, 1, 1], [2, 2, 2], [3, 3, 3]])\n",
        "\n",
        "sess = tf.Session()\n",
        "first_row_val, second_and_third_rows_val = sess.run(\n",
        "    [first_row, second_and_third_rows],  # Desired nodes to evaluate.\n",
        "    feed_dict={W_ph: W})  # Desired input.\n",
        "\n",
        "assert np.all(first_row_val == [[1, 1, 1]])\n",
        "assert np.all(second_and_third_rows_val == [[2, 2, 2], [3, 3, 3]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# In TensorFlow, a vector of dimension n is different than a matrix of dimension n x 1.\n",
        "# Sometimes, APIs require one or the other.  There are two useful functions...\n",
        "\n",
        "v = tf.placeholder(tf.float32)\n",
        "\n",
        "# Assume v is a matrix with a \"1\" sized dimension at the end (e.g. a 5 x 1 matrix).\n",
        "# Use tf.squeeze to remove the last dimension (e.g. turn that 5 x 1 matrix into a vector of length 5).\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# squeezed = \n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "squeezed_val = sess.run(squeezed, feed_dict={v: [[1], [2], [3]]})\n",
        "assert squeezed_val.shape == (3,)\n",
        "\n",
        "squeezed_val = sess.run(squeezed, feed_dict={v: [[[1], [2], [3]]]})\n",
        "#  Hint: when calling tf.squeeze, always provide \"axis\", explicitly stating which dimension to squeeze out.\n",
        "#  or you may have unintended side effects!\n",
        "assert squeezed_val.shape == (1, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "# In TensorFlow, a vector of dimension n is different than a matrix of dimension n x 1.\n",
        "# Sometimes, APIs require one or the other.  There are two useful functions...\n",
        "\n",
        "v = tf.placeholder(tf.float32)\n",
        "\n",
        "# Assume v is a vector (e.g. length 5).\n",
        "# Use tf.expand_dims to add a size 1 dimension (e.g. turn that 5 vector into a 5 x 1 matrix).\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# expanded = \n",
        "\n",
        "# END YOUR CODE HERE\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "expanded_val = sess.run(expanded, feed_dict={v: [1, 2, 3]})\n",
        "assert expanded_val.shape == (3, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**In this course, you'll realize that one of the most important debugging techniques is to draw pictures of all your tensors on a scrap piece of paper and make sure that it matches you code.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning in TensorFlow\n",
        "\n",
        "### Brief Review of Machine Learning\n",
        "\n",
        "In supervised learning, parametric models are those where the model is a function of a fixed form with a number of unknown _parameters_.  Together with a loss function and a training set, an optimizer can select parameters to minimize the loss with respect to the training set.  Common optimizers include stochastic gradient descent.  It tweaks the parameters slightly to move the loss \"downhill\" due to a small batch of examples from the training set.\n",
        "\n",
        "### Linear & Logistic Regression\n",
        "\n",
        "You've likely seen linear regression before.  In linear regression, we fit a line (technically, hyperplane) that predicts a target variable, $y$, based on some features $x$.  The form of this model is affine (even if we call it \"linear\"):  \n",
        "\n",
        "$$y_{hat} = xW + b$$\n",
        "\n",
        "where $W$ and $b$ are weights and an offset, respectively, and are the parameters of this parametric model.  The loss function that the optimizer uses to fit these parameters is the squared error ($||\\cdots||_2$) between the prediction and the ground truth in the training set.\n",
        "\n",
        "You've also likely seen logistic regression, which is tightly related to linear regression.  Logistic regression also fits a line - this time separating the positive and negative examples of a binary classifier.  The form of this model is similar: \n",
        "\n",
        "$$y_{hat} = \\sigma(xW + b)$$\n",
        "\n",
        "where again $W$ and $b$ are the parameters of this model, and $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) which maps un-normalized scores (\"logits\") to values $\\hat{y} \\in [0,1]$ that represent probabilities. The loss function that the optimizer uses to fit these parameters is the [cross entropy](../information_theory.ipynb) between the prediction and the ground truth in the training set.\n",
        "\n",
        "This pattern of an affine transform, $xW + b$, occurs over and over in machine learning.\n",
        "\n",
        "**We'll use logistic regression as our running example for the rest of this assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## B.  Short Answer Questions\n",
        "\n",
        "Imagine you want to implement logistic regression:\n",
        "\n",
        "* `z = xW + b`\n",
        "* `y_hat = sigmoid(z)`\n",
        "\n",
        "Where:\n",
        "1.  `x` is a 10-dimensional feature vector\n",
        "2.  `W` is the weight vector\n",
        "3.  `b` is the bias term\n",
        "\n",
        "What are the dimensions of `W` and `b`?  Recall that in logistic regression, `z` is just a scalar (commonly referred to as the \"logit\").\n",
        "\n",
        "Draw a picture of the whole equation using rectangles to illustrate the dimensions of `x`, `W`, and `b`.  See examples below for inspiration (though please label each dimension).  It's fine to do this part on paper and take a photo.  Name your photo b_answer.png."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching\n",
        "\n",
        "Let's say we want to perform inference using your model (parameters `W` and `b`) above on 10 examples intsead of just 1. On modern hardware (especially GPUs), we can do this efficiently by *batching*.\n",
        "\n",
        "To do this, we stack up the feature vectors in x like in the diagram below.  Note that changing the number of examples you run on (i.e. your batch size) *does not* affect the number of parameters in your model.  You're just running the same thing in parallel (instead of running the above one feature vector at a time at a time).\n",
        "\n",
        "![](batchaffine.png)\n",
        "\n",
        "The red (# features) and blue (batch size) lines represent dimensions that are the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C.  Short Answer Questions\n",
        "\n",
        "If we have 10 features and running the model in parallel with 20 examples, what are the dimensions of:\n",
        "\n",
        "1. `W` ?\n",
        "2. `b` ?\n",
        "3. `x` ?\n",
        "4. `z` ?\n",
        "\n",
        "_Hint:_ remember that your model parameters stay fixed!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## D.  Short Answer Questions\n",
        "\n",
        "Recall `y_hat = sigmoid(z)`.\n",
        "\n",
        "If you were to run your model one example of the time and `z = 0.8` for the first example and `z = -0.3` for the second example, that would result in `y_hat = 0.689` and `y_hat = 0.426`, respectively.\n",
        "\n",
        "If you run them in parallel/as a batch, the answers shouldn't change.\n",
        "\n",
        "Answer these questions:\n",
        "\n",
        "1.  What is the shape of `y_hat` when running in a batch, in terms of other dimensions.\n",
        "2.  What is the value of `y_hat` (as a vector) when running on the batch described earlier in this question?\n",
        "3.  Why do you think `sigmoid(vector)` is sometimes referred to as \"the sigmoid is applied element-wise\"?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## E.  Short Answer Questions\n",
        "\n",
        "In deep neural networks, there are often intermediate \"hidden layers\", a vector per example.\n",
        "\n",
        "Building on the batching in the previous example, the formulation looks like this:\n",
        "\n",
        "![](batchdnnaffine.png)\n",
        "\n",
        "Assuming we want a hidden layer size of 75 (continuing with 10 features and a batch size of 20 examples), what are the shapes of:\n",
        "1. W?\n",
        "2. b?\n",
        "3. x?\n",
        "4. z?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting it all together\n",
        "\n",
        "In all of these cases, the optimizer needs:\n",
        "\n",
        "- A batch of examples of $x$ and $y$ from the training set\n",
        "- Variables to maintain the current values of the parameters of the model\n",
        "- A loss function _(such as cross-entropy)_\n",
        "- An optimization strategy _(such as stochastic gradient descent (SGD))_\n",
        "\n",
        "## F. Coding exercise\n",
        "\n",
        "In this section, you don't need to create the graph and session (we've done it for you).  Instead, you will simply implement functions (in `graph.py`) that construct parts of a larger graph.\n",
        "\n",
        "You will first build an affine layer: \n",
        "\n",
        "$$z = xW + b$$\n",
        "\n",
        "and then then a stack of fully connected layers (described in more detail below), each implementing \n",
        "\n",
        "$$h^{(i)} = f(h^{(i-1)}W + b)$$\n",
        "\n",
        "You'll use the former as a building block for the latter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F.1 Affine Layer\n",
        "In particular, your function will accept a TensorFlow Op that represents the value of $x$ and should return value $z$ of desired dimension.  You must construct whatever variables you need.\n",
        "\n",
        "**In `graph.py`, implement `affine_layer(...)` **\n",
        "\n",
        "**Remember to take a photo** of the sketch we ask you to make in the function's comments.\n",
        "![Name your image f1_answer.png and rerun this cell](f1_answer.png)\n",
        "\n",
        "Hints:\n",
        "- use `tf.get_variable()` to create variables to store the current values of parameters.\n",
        "- `W` should be randomly initialized using [Xavier initialization](https://www.tensorflow.org/versions/master/api_docs/python/contrib.layers/initializers)\n",
        "- `b` should be initialized to a vector of zeros\n",
        "- `a * b` is a element-wise product, but what you'll want here is proper matrix multiplication (`tf.matmul`).\n",
        "\n",
        "Run the little fragment below until you get your code up and running, then run the more comprehensive unit tests in the cell below that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "with tf.Graph().as_default():\n",
        "    tf.set_random_seed(0)\n",
        "    sess = tf.Session()\n",
        "    x_ph = tf.placeholder(tf.float32, shape=(None, 3))\n",
        "    y = graph.affine_layer(1, x_ph)  #### <---- Your code called here.\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    print('You should have two trainable variables, one for each of parameters W and b: ', \n",
        "          len(tf.trainable_variables()))\n",
        "    assert len(tf.trainable_variables()) == 2\n",
        "\n",
        "    print('These should be a (3, 1) W weight matrix and a (1,) offset.')\n",
        "    variables = sess.run(tf.trainable_variables())\n",
        "    print(variables[0].shape)\n",
        "    print(variables[1].shape)\n",
        "    assert set([variables[0].shape, variables[1].shape]) == set([(3, 1), (1,)])\n",
        "\n",
        "    print('This should be [[-2.36877394]].')\n",
        "    y_val = sess.run(y, feed_dict={x_ph: np.array([[1, 2, 3]])})\n",
        "    print(y_val)\n",
        "    assert y_val.shape == (1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_affine', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F.2: Fully-Connected Layers\n",
        "\n",
        "A fully connected layer has the following form (you'll notice this is very similar to logistic regression!):\n",
        "\n",
        "1.  An affine transform $z^{(i)} = h^{(i-1)}W_i + b_i$\n",
        "2.  An elementwise nonlinearity $h^{(i)} = f(z^{(i)})$\n",
        "\n",
        "Logistic regression can be thought of as a single fully-connected layer where $f = \\sigma$ is a sigmoid. We'll use [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks) here instead, but the structure is otherwise the same.\n",
        "\n",
        "These fully connected layers can be stacked repeatedly to build a deep neural network:\n",
        "\n",
        "$$ \\begin{eqnarray} \n",
        "h^{(0)} & = &\\ x \\\\\n",
        "h^{(1)} & = &\\ f(z^{(1)})\\ =\\ f(h^{(0)}W_1 + b_1) \\\\\n",
        "h^{(2)} & = &\\ f(z^{(2)})\\ =\\ f(h^{(1)}W_2 + b_2) \\\\\n",
        "&\\ldots& \\\\\n",
        "h^{(i)} & = &\\ f(z^{(i)})\\ =\\ f(h^{(i-1)}W_i + b_i) \\\\\n",
        "\\end{eqnarray}$$\n",
        "\n",
        "**In `graph.py`, implement the `fully_connected_layers()` function.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_fully_connected_layers', graph_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_fully_connected_doesnt_use_hidden_dim_as_layer_name', graph_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_no_fully_connected_layers', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F.3 Compute logits\n",
        "\n",
        "Use the functions you've already implemented to build the computational graph taking features `x_ph` through a forward pass of a fully connected neural network with dimensions `hidden_dims` (a list of integers, like `[50, 35, 10]`).\n",
        "\n",
        "**In `graph.py`, implement `MakeLogits()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_make_logits', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F.4 Compute loss\n",
        "Given the logits and the labels, compute cross entropy loss.\n",
        "\n",
        "**In `graph.py`, implement `MakeLoss()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestLayer.test_make_loss', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F.5 Training a Neural Network\n",
        "\n",
        "Let's put it all together, and build a simple neural network that fits some training data.\n",
        "\n",
        "**Read the code for `train_nn()`, in `graph.py`**\n",
        "\n",
        "Note that much of this is boilerplate, but all the elements should be familiar from above: constructing the graph, feeding minibatches from NumPy arrays, and calling `session.run()`. On future assignments, we'll make use of the high-level [`tf.Estimator`](https://www.tensorflow.org/programmers_guide/estimators) API to abstract away some of this and allow us to focus on the model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph_test)\n",
        "X_train, y_train, X_test, y_test = graph_test.generate_data(1000, 10)\n",
        "plt.scatter(X_train[:,0], X_train[:,1], c=y_train, cmap='bwr');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Hint:** You should expect to see an initial loss here of 0.2 - 1.0.  This is because a well-initialized random classifier tends to output a uniform distribution.  For each example in the batch, we either compute the cross-entropy loss of the label (`[1, 0]` or `[0, 1]`) against the model's output (`~[0.5, 0.5]`).  Both cases result in $-\\ln(0.5) = ln(2) = 0.69$.\n",
        "\n",
        "Of course, your random classifier won't output exactly uniform distributions (it's random after all), but you should anticipate it being pretty close.  If it's not, your initialization may be broken and make it hard for your network to learn.\n",
        "\n",
        "**[Optional]** Some technical details... if your randomly initialized network is outputting very confident predictions, the loss computed may be very large while at the same time the sigmoids in the network are likely in saturation, quickly shrinking gradients.  The result is that you make tiny updates in the face of a huge loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestNN.test_train_nn', graph_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph)\n",
        "reload(graph_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestNN.test_train_nn_with_fclayers', graph_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That was fairly straightforward...  the data is clearly linearly separable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning Parameters\n",
        "\n",
        "Let's try our network on a problem that's a bit harder! This is a version of the classic XOR problem, which is not linearly-separable. However, it's easy to solve with a deep network.\n",
        "\n",
        "Here, we'll train a neural network with a couple of hidden layers before the final sigmoid.  This lets the network learn non-linear decision boundaries.\n",
        "\n",
        "Try playing around with the hyperparameters to get a feel for what happens if you set the learning rate too big (or too small), or if you don't give the network enough capacity (i.e. hidden layers and width)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(graph_test)\n",
        "X_train, y_train, X_test, y_test = graph_test.generate_non_linear_data(1000, 10)\n",
        "plt.scatter(X_test[:,0], X_test[:,1], c=y_test, cmap='bwr')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "hidden_layers = [10, 10]\n",
        "batch_size = 50\n",
        "epochs = 2000\n",
        "learning_rate = 0.001\n",
        "tf.reset_default_graph()\n",
        "predictions = graph.train_nn(X_train, y_train, X_test, hidden_layers, batch_size, epochs, learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:,0], X_test[:,1], c=predictions, cmap='bwr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That looks pretty good!\n",
        "\n",
        "Let's compare the predictions vs. the labels and see what we got wrong..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(X_test[:,0], X_test[:,1], c=(predictions!=y_test), cmap='bwr')\n",
        "print(\"Accuracy: {:.02f}%\".format(100*sum(predictions == y_test)/len(predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only a tiny number of errors (hopefully!).  Good work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Congratulations\n",
        "\n",
        "You have implemented a deep neural network using tensorflow!\n",
        "\n",
        "One remaining API you may want to take a look at is [tf.nn.embedding_lookup](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#embedding_lookup).  It is simply an op that takes a variable (like the \"w\" you did in your affine layer) and returns a column from it.  This will be useful later when we \"embed\" words into vector space.  We'll have our embedding table as a single variable with dimensions `[num_words x word_vector_length]` and we'll use this op to select word vectors from it efficiently."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
