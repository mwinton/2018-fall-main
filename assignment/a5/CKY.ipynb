{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phrase-structure parsing with CKY\n",
        "\n",
        "In this assignment, we'll train a simple phrase-structure (constituency) parser on the [Penn Treebank](https://catalog.ldc.upenn.edu/ldc99t42), then implement exact inference using the CKY algorithm.\n",
        "\n",
        "You may want to refer to the async material and lecture notes for a detailed discussion of the algorithm. The following may also be useful:\n",
        "- [Guide to Penn Treebank constituent tags](http://www.surdeanu.info/mihai/teaching/ista555-fall13/readings/PennTreebankConstituents.html) - explains what `JJ`, `NNP`, and all the other tags mean.\n",
        "- [Syntactic Parsing (Jurafsky & Martin, Ch. 12)](https://web.stanford.edu/~jurafsky/slp3/12.pdf) - particularly 12.1 and 12.2.\n",
        "- [Accurate Unlexicalized Parsing](http://ilpubs.stanford.edu:8091/~klein/unlexicalized-parsing.pdf) (Klein & Manning, 2003)\n",
        "\n",
        "We'll write code in three parts:\n",
        "- **(a)** Initial preprocessing of the treebank\n",
        "- **(b)** Calculation of production rule probabilities\n",
        "- **(c)** Implementation of CKY algorithm\n",
        "\n",
        "We provide the code for 1 and much of the framework surrounding 2 and 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Install a few python packages using pip\n",
        "from w266_common import utils\n",
        "utils.require_package(\"tqdm\")      # for progress bars\n",
        "utils.require_package(\"graphviz\")  # for rendering trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preliminaries: GraphViz\n",
        "\n",
        "**You can skip this if you have GraphViz set up.**\n",
        "\n",
        "This notebook uses [GraphViz](https://www.graphviz.org/) to render tree structures. On Ubuntu / Debian (including Google Cloud), you can install it by running on the command line:\n",
        "```\n",
        "sudo apt-get install graphviz\n",
        "```\n",
        "\n",
        "For Mac OSX, you can install using Homebrew:\n",
        "```\n",
        "brew install graphviz\n",
        "```\n",
        "or see https://www.graphviz.org/download/ for more options. Run the cell below to set up rendering and show a sample tree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nltk\n",
        "from w266_common import treeviz\n",
        "# Monkey-patch NLTK with better Tree display that works on Cloud or other display-less server.\n",
        "print(\"Overriding nltk.tree.Tree pretty-printing to use custom GraphViz.\")\n",
        "treeviz.monkey_patch(nltk.tree.Tree, node_style_fn=None, format='svg')\n",
        "\n",
        "# Test rendering\n",
        "print(\"Sample tree to test rendering:\")\n",
        "nltk.tree.Tree.fromstring(\"(S (NP (PRP I)) (VP (VBP love) (NNP W266)) (SYM ðŸ˜„))\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import some useful libraries...\n",
        "from __future__ import absolute_import\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import os, sys, collections\n",
        "import copy\n",
        "from importlib import reload\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "from IPython.display import display, HTML\n",
        "from tqdm import tqdm as ProgressBar\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(stream=sys.stderr, level=logging.DEBUG)\n",
        "\n",
        "# Helpers for this assignment\n",
        "from w266_common import utils, treeviz\n",
        "import cky_helpers\n",
        "import pcfg, pcfg_test\n",
        "import cky, cky_test\n",
        "\n",
        "# Use the sample of the Penn Treebank included with NLTK.\n",
        "assert(nltk.download('treebank'))\n",
        "corpus = nltk.corpus.treebank\n",
        "\n",
        "# If you do install the full Penn Treebank, change the \"False\" to \"True\" below.\n",
        "use_full_ptb = False\n",
        "if use_full_ptb:\n",
        "    cky_helpers.verify_ptb_install()\n",
        "    corpus = nltk.corpus.ptb  # Throws errors, for some reason\n",
        "    # This configures the corpus to use the WSJ section only.\n",
        "    # The Brown section has some mis-bracketed trees that will cause the \n",
        "    # corpus reader to throw (many) errors.\n",
        "    if not hasattr(corpus, '_parsed_sents'):\n",
        "        print(\"Monkey-patching corpus reader...\")\n",
        "        corpus._parsed_sents = corpus.parsed_sents\n",
        "        corpus.parsed_sents = lambda: corpus._parsed_sents(categories=['news'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (a) Exercises: API warm-up\n",
        "\n",
        "We'll be using `nltk.tree.Tree` objects in the rest of the assignment, which provide some convenient methods for viewing and traversing parse trees, as well as extracting grammar rules (productions).\n",
        "\n",
        "The API is documented here: http://www.nltk.org/api/nltk.html#nltk.tree.Tree\n",
        "\n",
        "In the cells below, do the following to familiarize yourself with the Tree API:\n",
        "\n",
        "1. Construct the tree `(S (NP foo) (VP bar))` using the Tree API. The constructor can be called as: `Tree(lhs, [rhs_1, rhs_2, ...]` where `lhs` is a string and `rhs_*` are either trees or strings.\n",
        "2. Display the tree for the first sentence in the corpus. (It should be: \"Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\")\n",
        "3. For the first sentence, print the label of the root node and the labels of its immediate children.\n",
        "4. For the first sentence, print all the productions in the sentence. Also print the LHS of the first production, and the RHS of the second production.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Part (a).1\n",
        "#### YOUR CODE HERE ####\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Part (a).2\n",
        "#### YOUR CODE HERE ####\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(Side node: it turns out this first sentence is actually pretty meta: http://languagelog.ldc.upenn.edu/nll/?p=3594 Long live Pierre Vinken!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Part (a).3\n",
        "#### YOUR CODE HERE ####\n",
        "s = corpus.parsed_sents()[0]\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Part (a).4\n",
        "#### YOUR CODE HERE ####\n",
        "s = corpus.parsed_sents()[0]\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (a) Preprocessing\n",
        "\n",
        "### Removing Cross-References\n",
        "\n",
        "This first step of preprocessing takes the treebank, strips out the cross references (NPs are wrapped by special nodes that assign index numbers to them so that coreferences can be indicated).  Unfortunately, this also injects a NP-SBJ-# node between nodes you'd expect to produce one another.  Since the # changes throughout the corpus, our counts of the production rules all end up being 1 - and useless.\n",
        "\n",
        "See NP-SBJ-1 in the tree below.  Note there is also a NP-SBJ leading to a NONE/1 subtree as a crossreference later.\n",
        "\n",
        "In the code below we skip over nodes whose label start with NP-, connecting any children nodes to the NP-'s parent.  We also snip out any subtrees rooted by NONE.  The tree above is printed again after this next cell to illustrate the effect of this code.\n",
        "\n",
        "### Chomsky Normal Form\n",
        "\n",
        "Finally, CKY assumes that trees are constructed from a grammar that is in [Chomsky normal form](https://en.wikipedia.org/wiki/Chomsky_normal_form).\n",
        "\n",
        "This means that the grammar only consists of three types of rules:\n",
        "- **Binary nonterminal:** `A -> B C`\n",
        "- **Unary preterminal:** `A -> a`\n",
        "- **Epsilon:** `A -> `$\\ \\ \\epsilon$\n",
        "\n",
        "where `A`, `B`, and `C`, are non-terminals, `a` is a terminal, and $\\epsilon$ is the empty sentence.\n",
        "\n",
        "In order to accomplish this, we add new non-terminals to the language and build longer sequences of non-terminals through them.   \n",
        "For example, the ternary rule\n",
        "- `A -> B C D`\n",
        "\n",
        "becomes two rules:\n",
        "- `A -> B A|<C-D>`\n",
        "- `A|<C-D> -> C D`\n",
        "\n",
        "where `A|<C-D>` is a dummy symbol that we add to signifiy that it's a production of `A` that creates `C D`.\n",
        "\n",
        "Since the resulting tree is (at most) binary, we sometimes call this process _binarization_.\n",
        "\n",
        "#### Horizontal Markovization\n",
        "\n",
        "The dummy-symbol system works well until you get very long grammar rules such as\n",
        "- `A -> B C D E F G H I J K L`\n",
        "\n",
        "If we followed the rule above, we'd get intermediate symbols that look like `A|<B-C-D-E-F-G-H-...>`. This would quickly lead to an explosion in the number of symbols in our grammar! Because such long productions are fairly rare, we may have trouble getting good estimates of their probability. (*Recall the sparsity problem from ngram language models!*)\n",
        "\n",
        "One way to counter this is called _horizontal Markovization_. Similar to how in language modeling, we \"forgot\" all history more than (n-1) words back, we can simply choose to truncate the the history and only store shorter symbols like `A|<B-C>`, `A|<C-D>`, `A|<D-E>`, and so on. This way, we can share parameters across more examples that are similar in structure.\n",
        "\n",
        "NLTK implements this for us in the `chomsky_normal_form` function; try changing the **`horzMarkov`** parameter below to see how it works.\n",
        "\n",
        "Take a minute to play with the ```horzMarkov``` parameter in the block below to see how this works. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "sentence = corpus.parsed_sents()[35]\n",
        "# Filter out NP-* nodes.\n",
        "cleaned_sentence = cky_helpers.clean_tree(sentence, simplify=True)\n",
        "# Convert sentence to Chomsky normal form.\n",
        "cnf_sentence = copy.deepcopy(cleaned_sentence)\n",
        "nltk.treetransforms.chomsky_normal_form(cnf_sentence, horzMarkov=2)\n",
        "display(HTML(treeviz.render_tree(cleaned_sentence, title=\"Original\", format='svg')))\n",
        "display(HTML(treeviz.render_tree(cnf_sentence, title=\"Binarized (CNF)\", format='svg')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run Pre-Processing on Corpus\n",
        "\n",
        "We'll loop through the whole corpus, and make copies of each sentence in CNF form. Use the `cnf_sentences` list for training the grammar in part (b).\n",
        "\n",
        "**Note:** if you're using the `treebank` corpus sample, this should run in just a few seconds. But if you use the full Penn Treebank, it'll take around 1-2 minutes to process all the trees. If you get an error \"`AttributeError: 'tqdm' object has no attribute 'miniters'`\", ignore it - the code should still work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Preprocess the treebank.\n",
        "cleaned_sentences = []\n",
        "cnf_sentences = []\n",
        "for sentence in ProgressBar(corpus.parsed_sents(), desc=\"Processing sentences\"):\n",
        "    # Filter out NP-* nodes.\n",
        "    cleaned_sentence = cky_helpers.clean_tree(sentence, simplify=True)\n",
        "    cleaned_sentences.append(cleaned_sentence)\n",
        "    \n",
        "    # Convert sentence to Chomsky normal form.\n",
        "    cnf_sentence = copy.deepcopy(cleaned_sentence)\n",
        "    nltk.treetransforms.chomsky_normal_form(cnf_sentence, horzMarkov=2)\n",
        "    cnf_sentences.append(cnf_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (b) Production rule probabilities\n",
        "\n",
        "In this next section, you'll compute about production rule probabilities.\n",
        "\n",
        "We won't use epsilon rules, so all of our rules will be of the form:\n",
        "- Binary nonterminal: `A -> B C`\n",
        "- Unary preterminal: `A -> a`\n",
        "\n",
        "The left hand side (LHS) of these rules only ever consist of a single nonterminal.  The right hand side (RHS) consists of either two non-terminals or one terminal.\n",
        "\n",
        "We'll do this in two stages:\n",
        "- Count LHS, and (LHS,RHS) each in their own dict\n",
        "- Calculate $ P(RHS | LHS) = \\frac{count(LHS, RHS)}{count(LHS)} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part (b) Implementation: Training the grammar\n",
        "\n",
        "Now that you're comfortable with NLTK Tree objects, let's use them to build our grammar. We've implemented a skeleton in **`pcfg.py`**; your job is to finish the implementation of the `pcfg.PCFG` class. Specifically:\n",
        "\n",
        "- Implement `update_counts`, which updates the production counts for a single sentence.\n",
        "- Implement `compute_scores`, which computes log-probabilities.\n",
        "\n",
        "Read the documentation in `pcfg.py` for the names of the data structures you should populate, and their precise types. Both functions should be straightforward, and only require a couple of lines of code each!\n",
        "\n",
        "#### Indexing Rules\n",
        "\n",
        "Your code here need only deal with straightforward maps of productions, but in order to parse efficiently we need to build an inverted index, keyed on the rule's RHS. This way, we can quickly look up rules (and their scores) that would combine two subtrees during the CKY algorithm.\n",
        "\n",
        "We've implemented this for you in `pcfg.PCFG.build_index()`, but you'll want to look carefully at how that function works - when you implement CKY in part(c), you'll make heavy use of the `grammar.parsing_index` structure.\n",
        "\n",
        "### Testing `update_counts()`\n",
        "\n",
        "If everything works, you should see this in the cell below:\n",
        "\n",
        "```\n",
        "Top productions:\n",
        "(PP -> IN NP, 7369)\n",
        "(, -> ',', 4885)\n",
        "(DT -> 'the', 4038)\n",
        "(. -> '.', 3828)\n",
        "(S|<VP-.> -> VP ., 3071)\n",
        "(NP -> NP PP, 2644)\n",
        "(S -> VP, 2335)\n",
        "(IN -> 'of', 2319)\n",
        "(TO -> 'to', 2161)\n",
        "(NP -> DT NN, 2020)\n",
        "```\n",
        "\n",
        "### Testing `compute_scores()`\n",
        "\n",
        "If everything went well, you should see:\n",
        "```\n",
        "food [(NN, -6.7128043057880404)]\n",
        "a [(DT, -1.4717815426061982), (JJ, -7.9783109698677208), (IN, -9.1959371416654392), (LS, -2.5649493574615367)]\n",
        "I [(NNP, -8.4563810520194806), (PRP, -2.7203634613355669)]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(pcfg)\n",
        "utils.run_tests(pcfg_test, [\"TestPCFG\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(pcfg)\n",
        "\n",
        "grammar = pcfg.PCFG()\n",
        "for sentence in ProgressBar(cnf_sentences, desc=\"Counting productions\"):\n",
        "    grammar.update_counts(sentence)\n",
        "    \n",
        "print(\"Top productions:\")\n",
        "for p in grammar.top_productions():  # Top productions, by un-normalized count\n",
        "    print(p)\n",
        "print(\"\")\n",
        "\n",
        "grammar.compute_scores()  # compute log-probabilities\n",
        "grammar.build_index()     # prepare for parsing\n",
        "\n",
        "for w in ['food', 'a', 'I']:\n",
        "    print(w, grammar.parsing_index[(w,)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You don't need to do anything with this next cell except to run it.\n",
        "\n",
        "It's not particularly useful, but if you need to keep track of what each variable contains, this provides a useful reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Productions (nltk.grammar.Production):')\n",
        "for (production, count) in grammar.production_counts.most_common(5):\n",
        "    print (production, count)\n",
        "\n",
        "print('\\n\\nLHS counts:')\n",
        "for (lhs, count) in grammar.lhs_counts.most_common(5):\n",
        "    print((lhs, count))\n",
        "    \n",
        "print('\\n\\nLog Probabilities:')\n",
        "print('\\n'.join([str(x) for x in grammar.parsing_index.items()][0:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# (c) Implement CKY\n",
        "\n",
        "After that bit of preamble, you only have one more task to go!  It's a big one though, so do take your time and get things right. \n",
        "\n",
        "**In `cky.py`, implement the `CKY_apply_preterminal_rules` and `CKY_apply_binary_rules` functions.**\n",
        "\n",
        "We've set up the skeleton of the CKY algorithm for you in the `CKY` function; be sure to read the in-line comments there carefully before you start. The outline is:\n",
        "1. Construct the chart (`make_chart`, provided)\n",
        "2. Populate the first row of the chart using preterminal rules (`CKY_apply_preterminal_rules`)\n",
        "3. Populate the rest of the chart using binary rules (`CKY_apply_binary_rules`)\n",
        "4. Read off the top cell for the final derivation. (provided)\n",
        "\n",
        "We'll implement the chart itself as a dict that you can index into first by cell position and then by non-terminal like this:\n",
        "```\n",
        "chart[(0, 1)][NN]\n",
        "```\n",
        "\n",
        "The value is an [nltk.tree.ProbabilisticTree](http://www.nltk.org/api/nltk.html#nltk.tree.ProbabilisticTree), which is just like an `nltk.tree.Tree` except that it has an additional `logprob()` method that returns the score (log-probability). Similarly, the constructor takes an additional argument: `pt = ProbabilisticTree(lhs, (rhs_1,...), logprob=score)` - you should use this to construct the backtrace trees.\n",
        "\n",
        "See the in-line comments in `cky.py` for additional hints and advice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(cky)\n",
        "utils.run_tests(cky_test, [\"TestParsing\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "reload(cky)\n",
        "parser = cky.CKYParser(grammar)\n",
        "derivation = parser.parse('I eat red hot food with a knife'.split(), 'S')\n",
        "\n",
        "assert round(derivation.logprob(), 2) == -64.08\n",
        "derivation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Experimentation\n",
        "\n",
        "Try a few more sentences.  Do you notice any patterns with your results?  Any common types of errors?  Are these an artifact of CKY, or of how you did the markovization/counting?\n",
        "\n",
        "Put any code you write in the next cell and a writeup of the results in the cell after.\n",
        "\n",
        "(If you have a format that's more natural to your description, feel free to deviate from this format.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "\n",
        "parser.parse('I eat some food'.split(), 'S')\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Your observation title 1\n",
        "Your observation detail.  (Copy and paste these two lines and fill them in for each observation.)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
