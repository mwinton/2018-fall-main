{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W266:  TensorFlow for Pedestrians - Towards A Neural Net in 3 Simple Steps\n",
    "\n",
    "This short Tensorflow Starter Notebook covers some very basic steps of TensorFlow. It is intended to be a introduction to the TensorFlow tutorial **\"W266: TensorFlow Tutorial\"**.\n",
    "\n",
    "The document aims to illustrate basic concepts of the framework step-by-step by utilizing the simplest of all examples: Linear Regression. In fact, after providing a brief general intro,  we will initially try to learn $w$ and $b$ for the model \n",
    "$$y = w x +b$$ \n",
    "where two data points x with their corresponding labels y are given. We do this first using gradient descend with manually calculated gradients, and then use a Tensorflow optimizer for gradient descent to do the work for us.\n",
    "\n",
    "We will then extend the problem to a simple neural net, adding  a hidden layer with one dimension.\n",
    "\n",
    "While these problems are very simple - the concept and many of the commands used are very similar to what you will see in  *\"W266: TensorFlow Tutorial\"* and in some of the homework sets.\n",
    "\n",
    "The structure of the notebook is as follows:\n",
    "\n",
    "\n",
    "<a name=\"TOC\">\n",
    "### Discussion Topics\n",
    "\n",
    "\n",
    "#### [1) Introduction:  Simple TensorFlow Calculations with Scalars and Tensors](#Intro)\n",
    "* [Basics](#Basics)   \n",
    "* [Simple Tensor Manipulations](#Tensors)   \n",
    "\n",
    "\n",
    "#### [2) Most Basic Linear Regression in Tensorflow: $y = wx + b$](#LR)\n",
    "* [Parallelizing calculations over a set of data points: Placeholders & Feed Dictionary](#Feeds)   \n",
    "* [Training... with manually calculated gradients](#Manual)  \n",
    "*  [Training... with a TensorFlow Optimizer](#Optimizer)    \n",
    "*  [Predictions & Model Save/Restore  ](#Preds)    \n",
    " \n",
    "\n",
    "#### [3) Slightly Fancier: A Simple Neural Net](#NN)\n",
    "\n",
    "#### [Appendix A: Visualization with Tensorboard](#TB)\n",
    "\n",
    "#### [Appendix B: A Better way to Manage Variables: Name Scopes & get_variable](#Vars)\n",
    "\n",
    "Useful References:\n",
    "\n",
    "** 1) [Getting Started with TensorFlow](https://www.tensorflow.org/get_started/get_started)**    \n",
    "** 2) \"Hands-On Machine Learning with Scikit-Learn and TensorFlow\", Aurélien Géron **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Intro\">\n",
    "## 1) Introduction:  Simple TensorFlow Calculations with Scalars and Tensors\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "<a name=\"Basics\">\n",
    "### Basics\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "\n",
    "To get started, with an eye on the Linear Regression Model we want to train in the next section, let us define three scalars $x, w, b$ and see whether we can properly calculate $ y = w * x + b$. To get used to some TensorFlow commands, we define y in two equivalent ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 basic phases: 1) define the graph that TensorFlow will compute, and 2) actually run it.\n",
    "\n",
    "First, we define a very simple graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()           # all calculations are represented as graphs. \n",
    "                                   # This command resets the default graph.\n",
    "\n",
    "x = tf.constant(3, name = \"x\")     # x is a constant. It will be fixed \n",
    "\n",
    "w = tf.Variable(1, name = \"w\")     # w and b are variables, i.e. they could be updated. \n",
    "b = tf.Variable(1, name = \"b\")\n",
    "\n",
    "y_1 = w * x  + b\n",
    "y_2 = tf.add(w * x, b)             # identical to previous statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have defined $x$ as a *constant* and $w$ and $b$ as *variables*. Constants are just what the name implies, while variables can be changed during execution. So the latter are the natural choice for weights that we (later) want to learn. (We will later use a different, and better, way to define variables.)\n",
    "\n",
    "Now, we run the session to compute what we are interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_1:  4\n",
      "result_2:  4\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    result_1 = sess.run(y_1)\n",
    "    result_2 = y_2.eval()     # equivalent to first format\n",
    "    \n",
    "\n",
    "print('result_1: ', result_1)\n",
    "print('result_2: ', result_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. The first TensorFlow calculations are working!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above method is quite inefficient as the whole graph is calculated twice, once for each evaluation. Can we do this more efficiently? We certainly can by passing the list of to-be-calculated/executed operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result_1:  4\n",
      "result_2:  4\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    result_1, result_2 = sess.run([y_1, y_2])\n",
    "\n",
    "print('result_1: ', result_1)\n",
    "print('result_2: ', result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is better.\n",
    "\n",
    "So much for scalars. TensorFlow, as the name suggests, generally deals with tensors. Therefore, it is useful to look at some basic tensor manipulations and calculations.\n",
    "\n",
    "<a name=\"Tensors\">\n",
    "### Simple Tensor Manipulations\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "\n",
    "Roughly speaking, a tensor is simply an order $n$ generalization of a matrix. \n",
    "\n",
    "We start with the simplest tensor: a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_1:0' shape=(3,) dtype=int32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "v1 = tf.constant([1,2,3])\n",
    "v2 = tf.constant([4,5,6])\n",
    "v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So $v_1$ has one index.\n",
    "\n",
    "Useful operations are dot products and the 'square', i.e. the dot product of a vector with itself (or element-wise squaring): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prod = tf.tensordot(v1, v2, axes=1)\n",
    "v1_square = tf.square(v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1:  [1 2 3]\n",
      "v2:  [4 5 6]\n",
      "prod:  32\n",
      "v1_square:  [1 4 9]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    v1_calc, v2_calc, prod_calc, v1_square_calc = sess.run([v1,v2,prod,v1_square])\n",
    "    \n",
    "print('v1: ', v1_calc)\n",
    "print('v2: ', v2_calc)\n",
    "print('prod: ', prod_calc)\n",
    "print('v1_square: ', v1_square_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy inputs work as well and are usually used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v3:  [1 4 6]\n"
     ]
    }
   ],
   "source": [
    "v3 = tf.constant(np.asarray([1,4,6]))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    v = sess.run(v3)\n",
    "    \n",
    "print('v3: ', v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the expected values.\n",
    "\n",
    "Next, we continue with matrices - transpose, matrix multiplication, and the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format of a:  Tensor(\"Const:0\", shape=(2, 3), dtype=int32)\n",
      "Format of b:  Tensor(\"transpose:0\", shape=(3, 2), dtype=int32)\n",
      "Format of c:  Tensor(\"MatMul:0\", shape=(2, 2), dtype=int32)\n",
      "Format of d:  Tensor(\"Trace:0\", shape=(), dtype=int32)\n",
      "Format of e:  Tensor(\"Square:0\", shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "a = tf.constant([[1,2,3],[4,5,6]])\n",
    "b = tf.transpose(a)\n",
    "c = tf.matmul(a, tf.transpose(a))   # note the transpose to match matrix indices\n",
    "d = tf.trace(c)\n",
    "e = tf.square(c)\n",
    "\n",
    "print('Format of a: ', a)\n",
    "print('Format of b: ', b)\n",
    "print('Format of c: ', c)\n",
    "print('Format of d: ', d)\n",
    "print('Format of e: ', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we could not define *tf.matmul(a,a)* because the indices need to match. An $m\\times n$ matrix can only be multiplied by a $n\\times s$ matrix.\n",
    "\n",
    "These statements above again merely define the objects, but no calculations are made.  We can however see the dimensions of the objects in question, and those are correct.\n",
    "\n",
    "The code below - which should be obvious now - is calculating the values of the objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix: a  [[1 2 3]\n",
      " [4 5 6]]\n",
      "\n",
      "Transpose: b=  [[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n",
      "\n",
      "m x m^t:  c=  [[14 32]\n",
      " [32 77]]\n",
      "\n",
      "Trace of c: d=  91\n",
      "\n",
      "Point-wise square of c: e=  [[ 196 1024]\n",
      " [1024 5929]]\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    a_calc, b_calc, c_calc, d_calc, e_calc = sess.run([a,b,c,d,e])\n",
    "    \n",
    "print('Original matrix: a ', a_calc)\n",
    "print('\\nTranspose: b= ', b_calc)\n",
    "print('\\nm x m^t:  c= ', c_calc)\n",
    "print('\\nTrace of c: d= ', d_calc)\n",
    "print('\\nPoint-wise square of c: e= ', e_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the expected results. \n",
    "\n",
    "Next, let us look at re-shaping of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Reshape_1:0\", shape=(3, 2), dtype=int32)\n",
      "Tensor(\"Reshape_2:0\", shape=(1, 6), dtype=int32)\n",
      "Tensor(\"Reshape_3:0\", shape=(3, 1, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(a)                          # will show original shape (2,3)\n",
    "print(tf.reshape(a, [3,2]))       # will show new shape (3,2)           \n",
    "print(tf.reshape(a, [-1,6]))      # '-1' is replaced with the proper dimension. This will show shape (1,6)\n",
    "print(tf.reshape(a, [3, 1, -1]))  # an this will result in a tensor of shape (3,1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_4:0\", shape=(3, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "v = tf.reshape(a, [3,2])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, out of interest (this will not be used in this tutorial), how does tf.matmul work with tensors? Here is the answer from the TensorFlow documentation:\n",
    "\n",
    ">output[..., i, j] = sum_k (a[..., i, k] * b[..., k, j]), for all indices i, j.\n",
    "\n",
    "Let's test that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape_5:0\", shape=(1, 3, 1, 2), dtype=int32)\n",
      "Tensor(\"Reshape_6:0\", shape=(1, 3, 2, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "s = tf.reshape(a, [1, 3, 1, 2])\n",
    "t = tf.reshape(a, [1, 3, 2, 1])\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MatMul_1:0' shape=(1, 3, 1, 1) dtype=int32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.matmul(s,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very good, this was correct. Both factors, after suitable transposition, etc., need to have identical index structure for the first $n-2$ indices, and then the last index of the first factor needs to match the 2nd to last index of the 2nd factor.\n",
    "\n",
    "We have now the foundations to define and train a very simple linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"LR\">\n",
    "## 2) Most Basic Linear Regression in TensorFlow: $y = wx + b$\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "With some basics out of the way, we can now proceed with a very simple Linear Regression.\n",
    "\n",
    "<a name=\"Feeds\">\n",
    "### Parallelizing calculations over a set of data points: Placeholders & Feed Dictionary\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "\n",
    "First, we need to address how to best deal with the data in training and test sets.\n",
    "In practice, we will have many different data points and including their values at the outside is not practical. We also may only want to use the data in batches. How can we first define x 'in general', and then feed in the actual values later at run-time?\n",
    "\n",
    "The solution is to replace the definition of x with a **placeholder** and define a **feed dictionary** that contains the actual values. The placeholder does only specify the shape and type of the variables. \n",
    "\n",
    "What is the correct shape for our $x$-placeholder? For the type of problem at hand (linear regression), it is (*batch size*) x *(number of features)*. Since at this point the batch size is unknown, the corresponding dimension is set to *None* in the placeholder. \n",
    "\n",
    "The number of features in our simple case is 1 and so we choose $X$ to be a *(batch size) x 1* matrix. We also interpret $w$ and $b$ as 1x1 matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")    # a placeholder is defined\n",
    "\n",
    "w = tf.Variable([[1.0]],name = \"w\")\n",
    "b = tf.Variable([[1.0]], name = \"b\")\n",
    "\n",
    "Y = X * w + b\n",
    "\n",
    "Y = tf.matmul(X, w) + b  # for the 1-feature base case equivalent to Y = X * w + b, \n",
    "                         # but in general appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the more general case with more than 1 feature, the rows of $X$ would correspond the individual data point and the columns would capture the features. Also, $w$ would need to be defined as a column vector in this case, otherwise a tf.transpose would need to be used.\n",
    "\n",
    "Next, we test whether the numbers work out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 4.],\n",
      "       [ 6.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "feedDict = {X:[[3], [5]]}\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    result = sess.run([Y], feed_dict=feedDict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, these are obviously the correct values and we can start the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Manual\">\n",
    "### Training... with manually calculated gradients\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "\n",
    "That was not hard. But we had to know and define $w$ and $b$. In general, we would have a training set with $x$ and known labels $y$, and we want to learn the weights/parameters of the model, in this case $w$ and $b$. How can that be done in TensorFlow? \n",
    "\n",
    "We need to introduce an iterative learning procedure, where in each step the trainable variables are updated. We begin by  defining the updates for $w$ and $b$ using gradients that we can easily calculate manually using gradient descent of the cost function $C$. Briefly recalling the gradient descent update procedure from W207:\n",
    "\n",
    "$$ w \\rightarrow w - \\alpha \\frac{\\partial}{\\partial_w} C  ; \\\\    \n",
    "b \\rightarrow b - \\alpha \\frac{\\partial}{\\partial_b} C $$\n",
    "\n",
    "where $\\alpha$ is the learning rate. With \n",
    "$$C = \\frac{1}{2}\\sum_i (y^i - y^i_{pred}) \\times (y^i - y^i_{pred})   = \\frac{1}{2}\\sum_i (y^i - w x^i -b) \\times (y^i - w x^i -b)  $$ we find:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial_w} C=  -\\sum_i (y^i - w x^i -b) x^i = \\sum_i (y_{pred}^i - y^i) x^i ; \\\\  \n",
    "\\frac{\\partial}{\\partial_b} C  = -\\sum_i (y^i - w x^i -b) = \\sum_i (y_{pred}^i - y^i)  $$\n",
    "\n",
    "\n",
    "Now we have everything. Let's start to implement:\n",
    "\n",
    "**a) Define placeholders for training set, initialize your weights, and define model: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")\n",
    "Y = tf.placeholder(tf.float32, shape=(None,1), name = \"y\")          # we will feed in 'labels' as well\n",
    "\n",
    "w = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w\")\n",
    "b = tf.Variable(tf.random_uniform(shape=(1,)), name = \"b\")\n",
    "\n",
    "Y_pred = tf.matmul(X,tf.transpose(w)) + b  # This is the generalizable way to write y = w*x + b to allow \n",
    "                                           # for more features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Define error and cost function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "error = Y_pred - Y\n",
    "mse = 0.5 * tf.reduce_mean(tf.square(error), name = \"mse\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Define learning rate, gradients, and gradient descent training steps:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "grad_w =  tf.matmul(tf.transpose(X) , error)\n",
    "grad_b =  tf.reduce_sum(error) \n",
    "\n",
    "train_w = tf.assign(w,w - learning_rate * grad_w)\n",
    "train_b = tf.assign(b, b - learning_rate * grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.assign tells TensorFlow to set the variable on the left to the value on the right upon execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Define the Feed Dictionary. Include the training y labels as well:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Train! (Don't forget to compute the training operation...)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\terror:  7.32071\n",
      "\tcurrent w:  [[ 0.54205137]]\n",
      "\tcurrent b [ 0.42031857]\n",
      "Epoch:  501\n",
      "\terror:  0.0019286\n",
      "\tcurrent w:  [[ 1.0603776]]\n",
      "\tcurrent b [ 0.74423653]\n",
      "Epoch:  1001\n",
      "\terror:  0.000632294\n",
      "\tcurrent w:  [[ 1.03457141]]\n",
      "\tcurrent b [ 0.85355324]\n",
      "Epoch:  1501\n",
      "\terror:  0.000207307\n",
      "\tcurrent w:  [[ 1.01979518]]\n",
      "\tcurrent b [ 0.91614646]\n",
      "Epoch:  2001\n",
      "\terror:  6.79674e-05\n",
      "\tcurrent w:  [[ 1.01133466]]\n",
      "\tcurrent b [ 0.9519859]\n",
      "Epoch:  2501\n",
      "\terror:  2.22838e-05\n",
      "\tcurrent w:  [[ 1.00649011]]\n",
      "\tcurrent b [ 0.9725076]\n",
      "Epoch:  3001\n",
      "\terror:  7.30544e-06\n",
      "\tcurrent w:  [[ 1.00371623]]\n",
      "\tcurrent b [ 0.98425788]\n",
      "Epoch:  3501\n",
      "\terror:  2.39583e-06\n",
      "\tcurrent w:  [[ 1.00212812]]\n",
      "\tcurrent b [ 0.99098557]\n",
      "Epoch:  4001\n",
      "\terror:  7.85369e-07\n",
      "\tcurrent w:  [[ 1.00121868]]\n",
      "\tcurrent b [ 0.994838]\n",
      "Epoch:  4501\n",
      "\terror:  2.57505e-07\n",
      "\tcurrent w:  [[ 1.00069785]]\n",
      "\tcurrent b [ 0.99704373]\n",
      "\n",
      "Final values:\n",
      "\tepochs:  5000\n",
      "\terror:  8.47195e-08\n",
      "\tw:  [[ 1.00040007]]\n",
      "\tb:  [ 0.99830472]\n",
      "\tPredicted y:  [[ 3.99950457]\n",
      " [ 6.00030565]]\n",
      "\tActual y:  [[4], [6]]\n"
     ]
    }
   ],
   "source": [
    "reportStep = 500\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        w_c, b_c,_, _, y_result, mse_result = \\\n",
    "        sess.run([w,b,train_w, train_b,   Y_pred, mse],feed_dict=feedDict)  # note the training steps for w and b!\n",
    "        #w_c, b_c, y_result, mse_result = \\\n",
    "        #sess.run([w,b,  Y_pred, mse],feed_dict=feedDict)  # note the training steps for w and b!\n",
    "    \n",
    "        if (epoch ) % reportStep == 0:\n",
    "            print('Epoch: ', epoch+1)\n",
    "            print('\\terror: ',mse_result)\n",
    "            print('\\tcurrent w: ', w_c)\n",
    "            print('\\tcurrent b', b_c)\n",
    "    print('\\nFinal values:')\n",
    "    print('\\tepochs: ',epoch+1)\n",
    "    print('\\terror: ',mse_result)\n",
    "    print('\\tw: ', w_c)\n",
    "    print('\\tb: ', b_c)\n",
    "    print('\\tPredicted y: ', y_result)\n",
    "    print('\\tActual y: ', [[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. So that is how TensorFlow learns. But calculating derivatives manually is obviously not a sustainable approach.\n",
    "\n",
    "<a name=\"Optimizer\">\n",
    "### Training... with a TensorFlow Optimizer\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "Gradient Descent is certainly a method that is anything but uncommon so it would be surprising if TensorFlow didn't have a framework where it calculates the derivatives and defines the training procedure for us. And indeed, **Optimizers** are doing the job.\n",
    "\n",
    "The procedure is essentially the same as above, except that we change the learning steps to use TensorFlow's Gradient Descent optimizer.\n",
    "\n",
    "** Repeat steps a) and b) from above:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")\n",
    "Y = tf.placeholder(tf.float32, shape=(None,1), name = \"y\")\n",
    "\n",
    "w = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w\")\n",
    "b = tf.Variable(tf.random_uniform(shape=(1,)), name = \"b\")\n",
    "\n",
    "Y_pred = tf.matmul(X,tf.transpose(w)) + b\n",
    "\n",
    "error = Y_pred - Y\n",
    "mse = 0.5 * tf.reduce_mean(tf.square(error), name = \"mse\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the manually calculated gradients and enforcing an assign-step at each iteration that updates $w$ and $b$ at every step, we'll leave this work to TensorFlow by choosing an optimizer and defining a new training step, replacing gradient calculation and the iterative re-definition of our weights/biases with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_all = optimizer.minimize(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are multiple optimizers to choose from. We used the simple gradient descent one.\n",
    "\n",
    "From here on, the steps are exactly as before in steps d) and e):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\terror:  0.312416\n",
      "\tcurrent w:  [[ 0.98956263]]\n",
      "\tcurrent b [ 0.38830844]\n",
      "Epoch:  501\n",
      "\terror:  0.00561621\n",
      "\tcurrent w:  [[ 1.10309064]]\n",
      "\tcurrent b [ 0.56330132]\n",
      "Epoch:  1001\n",
      "\terror:  0.00321629\n",
      "\tcurrent w:  [[ 1.07801414]]\n",
      "\tcurrent b [ 0.66952699]\n",
      "Epoch:  1501\n",
      "\terror:  0.00184189\n",
      "\tcurrent w:  [[ 1.05903757]]\n",
      "\tcurrent b [ 0.74991304]\n",
      "Epoch:  2001\n",
      "\terror:  0.00105482\n",
      "\tcurrent w:  [[ 1.04467738]]\n",
      "\tcurrent b [ 0.81074411]\n",
      "Epoch:  2501\n",
      "\terror:  0.000604077\n",
      "\tcurrent w:  [[ 1.03381002]]\n",
      "\tcurrent b [ 0.85677898]\n",
      "Epoch:  3001\n",
      "\terror:  0.00034595\n",
      "\tcurrent w:  [[ 1.02558613]]\n",
      "\tcurrent b [ 0.89161563]\n",
      "Epoch:  3501\n",
      "\terror:  0.000198122\n",
      "\tcurrent w:  [[ 1.01936281]]\n",
      "\tcurrent b [ 0.91797793]\n",
      "Epoch:  4001\n",
      "\terror:  0.000113468\n",
      "\tcurrent w:  [[ 1.01465321]]\n",
      "\tcurrent b [ 0.93792802]\n",
      "Epoch:  4501\n",
      "\terror:  6.49831e-05\n",
      "\tcurrent w:  [[ 1.01108944]]\n",
      "\tcurrent b [ 0.95302516]\n",
      "\n",
      "Final values:\n",
      "\tepochs:  5000\n",
      "\terror:  3.7261e-05\n",
      "\tw:  [[ 1.00839698]]\n",
      "\tb:  [ 0.96443021]\n",
      "\tPredicted y:  [[ 3.9896152]\n",
      " [ 6.0064187]]\n",
      "\tActual y:  [[4], [6]]\n"
     ]
    }
   ],
   "source": [
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "reportStep = 500\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        w_c, b_c,_,  y_result, mse_result = sess.run([w,b,train_all, Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "        if (epoch ) % reportStep == 0:\n",
    "            print('Epoch: ', epoch+1)\n",
    "            print('\\terror: ',mse_result)\n",
    "            print('\\tcurrent w: ', w_c)\n",
    "            print('\\tcurrent b', b_c)\n",
    "    print('\\nFinal values:')\n",
    "    print('\\tepochs: ',epoch+1)\n",
    "    print('\\terror: ',mse_result)\n",
    "    print('\\tw: ', w_c)\n",
    "    print('\\tb: ', b_c)\n",
    "    print('\\tPredicted y: ', y_result)\n",
    "    print('\\tActual y: ', [[4],[6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple! In various shapes or forms, this structure will be used in the class many times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Preds\">\n",
    "### Predictions & Model Save/Restore \n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Now where we have trained a model, we also want to make predictions. There are multiple obvious ways, and we will discuss two here.\n",
    "\n",
    "First off, one can simply define a second feed dictionary for the test values for $X$, and feed that into a new calculation at the very end within the same session: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model values:\n",
      "\tw:  [[ 1.00521314]]\n",
      "\tb:  [ 0.97791725]\n",
      "\n",
      "Predicted for [[7],[9]]:  [[  8.01440907]\n",
      " [ 10.02483559]]\n",
      "Expected for [[7],[9]]: [[8], [10]]\n",
      "Manually calculated from y = wx +b:\n",
      "\t[[ 8.01440907]], [[ 10.02483559]]\n"
     ]
    }
   ],
   "source": [
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "feedDict_test = {X:[[7], [9]]}                              # feed dictionary for test values\n",
    "\n",
    "reportStep = 500\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        w_c, b_c,_,  y_result, mse_result = sess.run([w,b,train_all, Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "\n",
    "    print('\\nFinal model values:')\n",
    "\n",
    "    print('\\tw: ', w_c)\n",
    "    print('\\tb: ', b_c)\n",
    "\n",
    "    \n",
    "    y_test = sess.run(Y_pred,feed_dict=feedDict_test)         # calculation of test Y\n",
    "    print('')\n",
    "    print('Predicted for [[7],[9]]: ', y_test)\n",
    "    print('Expected for [[7],[9]]: [[8], [10]]')\n",
    "    print('Manually calculated from y = wx +b:')\n",
    "    print('\\t%s, %s' % (7*w_c + b_c, 9 * w_c +b_c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that this simple approach calculated the actual predictions. But we needed to stay within the same session.\n",
    "\n",
    "A better way is to save and restore the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final model values:\n",
      "\tw:  [[ 1.01206577]]\n",
      "\tb:  [ 0.9488886]\n",
      "Saving model...\n",
      "Done!\n",
      " \n",
      "Restoring model in new session...\n",
      "INFO:tensorflow:Restoring parameters from modelSave/model_1.ckpt\n",
      "\n",
      "Predicted for [[7],[9]]:  [[  8.03334904]\n",
      " [ 10.05748081]]\n",
      "Expected for [[7],[9]]: [[8], [10]]\n",
      "Manually calculated from y = wx +b:\n",
      "\t[[ 8.03334904]], [[ 10.05748081]]\n"
     ]
    }
   ],
   "source": [
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "reportStep = 500\n",
    "\n",
    "saver  = tf.train.Saver()                                # Define a 'saver'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        w_c, b_c,_,  y_result, mse_result = sess.run([w,b,train_all, Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "\n",
    "    print('\\nFinal model values:')\n",
    "\n",
    "    print('\\tw: ', w_c)\n",
    "    print('\\tb: ', b_c)\n",
    "    \n",
    "    print('Saving model...')\n",
    "    save = saver.save(sess, 'modelSave/model_1.ckpt')      # Save model parameters to file\n",
    "    print('Done!')\n",
    "    print(' ')\n",
    "\n",
    "    \n",
    "    \n",
    "feedDict_test = {X:[[7], [9]]} \n",
    "\n",
    "with tf.Session() as sess:                                # This is a new session!\n",
    "    \n",
    "    print('Restoring model in new session...')\n",
    "    saver.restore(sess, 'modelSave/model_1.ckpt')         # Restore the weights\n",
    "    \n",
    "    y_test = sess.run(Y_pred,feed_dict=feedDict_test)\n",
    "    print('')\n",
    "    print('Predicted for [[7],[9]]: ', y_test)\n",
    "    print('Expected for [[7],[9]]: [[8], [10]]')\n",
    "    print('Manually calculated from y = wx +b:')\n",
    "    print('\\t%s, %s' % (7*w_c + b_c, 9 * w_c +b_c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"NN\">\n",
    "## 3) Slightly Fancier: A Simple Neural Net\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "\n",
    "We can now apply what we learned and define a first neural network. The architecture is very simple: we have one input neuron, a hidden layer of dimension 1 (plus bias), and then the 1-d output Y. The logic is exactly as above, we just need to incorporate the hidden layer.\n",
    "\n",
    "As before, we define placeholders for $X$ and $Y$ and define $w_1$ and $b_1$ ( we will now have a 2nd set of $w/b$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")  \n",
    "Y = tf.placeholder(tf.float32, shape=(None,1), name = \"y\")    \n",
    "\n",
    "w_1 = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w_1\")    \n",
    "b_1 = tf.Variable(tf.random_uniform(shape = (1,)),name = \"b_1\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the hidden layer L. This step is very similar to the definition of what used to be Y_pred, except that we will also add an activation function, *relu* in this case.\n",
    "\n",
    "[Also, we could choose to rewrite: \n",
    "\n",
    "(tf.matmul(X, w_1) +  b_1) $\\rightarrow$ (tf.nn.bias_add(tf.matmul(X, w_1), b_1))\n",
    "\n",
    "This notation is better aligned with adding a bias term in more general neural nets, but here it is equivalent.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = tf.matmul(X, w_1) + b_1     # a better notation would be:  L = tf.nn.bias_add(tf.matmul(X, w_1), b_1)\n",
    "L = tf.nn.relu(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_2 = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w_2\")  # define weight for hidden layer -> output\n",
    "b_2 = tf.Variable(tf.random_uniform(shape = (1,)),name = \"b_2\")  # define bias for hidden layer -> output   \n",
    "              \n",
    "Y_pred = tf.matmul(L, w_2) + b_2                                 # a better notation would be \n",
    "                                                                 # Y_pred = tf.nn.bias_add(tf.matmul(L, w_2), b_2) \n",
    "\n",
    "error = Y_pred - Y                                               # This is now a (batch size) vector\n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")          \n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_all = optimizer.minimize(mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\terror:  14.9238\n",
      "Epoch:  501\n",
      "\terror:  0.000153542\n",
      "Epoch:  1001\n",
      "\terror:  2.07415e-05\n",
      "Epoch:  1501\n",
      "\terror:  2.79496e-06\n",
      "Epoch:  2001\n",
      "\terror:  3.76679e-07\n",
      "Epoch:  2501\n",
      "\terror:  5.0781e-08\n",
      "Epoch:  3001\n",
      "\terror:  6.79267e-09\n",
      "Epoch:  3501\n",
      "\terror:  9.01423e-10\n",
      "Epoch:  4001\n",
      "\terror:  1.36652e-10\n",
      "Epoch:  4501\n",
      "\terror:  3.69482e-11\n",
      "\n",
      "Final values:\n",
      "\tepochs:  5000\n",
      "\terror:  2.10321e-11\n",
      "\tPredicted Y:  [[ 4.00000525]\n",
      " [ 5.99999619]]\n",
      "\tActual y:  [[4], [6]]\n",
      "\tw_1:  [[ 1.05612111]]\n",
      "\tb_1:  [ 0.38839042]\n",
      "\tw_2:  [[ 0.94685668]]\n",
      "\tb_2:  [ 0.63226956]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reportStep = 500\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        _, y_result, mse_result = sess.run([train_all,  Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "        if (epoch ) % reportStep == 0:\n",
    "            print('Epoch: ', epoch+1)\n",
    "            print('\\terror: ',mse_result)\n",
    "            \n",
    "    w_1_calc, w_2_calc, b_1_calc,b_2_calc,y_pred_calc,mse_calc = \\\n",
    "        sess.run([w_1, w_2, b_1, b_2,  Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "    print('\\nFinal values:')\n",
    "    print('\\tepochs: ',epoch+1)\n",
    "    print('\\terror: ',mse_result)\n",
    "    print('\\tPredicted Y: ', y_result)\n",
    "    print('\\tActual y: ', [[4],[6]])\n",
    "    print('\\tw_1: ',w_1_calc)\n",
    "    print('\\tb_1: ',b_1_calc)\n",
    "    print('\\tw_2: ',w_2_calc)\n",
    "    print('\\tb_2: ',b_2_calc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needless to say, with two data points we are heavily over-training here, but that is not the point of the exercise.\n",
    "\n",
    "This concludes the main part of this brief introduction into TensorFlow. In the appendix we will talk about a few useful topics that help with building, training, and testing Tensorflow models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"TB\">\n",
    "## Appendix A) Visualization with Tensorboard\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "Tensorboard is a great way to visualize i) the graph, and ii) the values of various parameters like weights or loss function over time. This is a good way to debug and optimize the model. (But don't write variables too often as it will impact performance.)\n",
    "\n",
    "Key steps include:\n",
    "\n",
    "1) define a log directory, where the values are written to    \n",
    "2) define a file writer (similar to \"myFile = open(file, 'w')\" in straight python.)\n",
    "3) define 'summaries' for the parameters to track      \n",
    "4) at various intervals, calculate the summaries and write them to the file.   \n",
    "\n",
    "After starting Tensorboard (tensorboard --logdir \\[logdir\\]), the visualziations can then (by defuault) be found at localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1\n",
      "\terror:  16.3699\n",
      "Epoch:  501\n",
      "\terror:  0.000506673\n",
      "Epoch:  1001\n",
      "\terror:  0.000166116\n",
      "Epoch:  1501\n",
      "\terror:  5.44654e-05\n",
      "Epoch:  2001\n",
      "\terror:  1.78582e-05\n",
      "Epoch:  2501\n",
      "\terror:  5.85509e-06\n",
      "Epoch:  3001\n",
      "\terror:  1.9199e-06\n",
      "Epoch:  3501\n",
      "\terror:  6.29899e-07\n",
      "Epoch:  4001\n",
      "\terror:  2.06716e-07\n",
      "Epoch:  4501\n",
      "\terror:  6.78303e-08\n",
      "\n",
      "Final values:\n",
      "\tepochs:  5000\n",
      "\terror:  2.21917e-08\n",
      "\tPredicted Y:  [[ 3.99982071]\n",
      " [ 6.00011063]]\n",
      "\tActual y:  [[4], [6]]\n",
      "\tw:  [[ 1.00014508]]\n",
      "\tb:  [ 0.99938571]\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")  \n",
    "Y = tf.placeholder(tf.float32, shape=(None,1), name = \"y\")    \n",
    "\n",
    "w = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w\")    \n",
    "b = tf.Variable(tf.random_uniform(shape = (1,)),name = \"b\")    \n",
    "\n",
    "Y_pred = tf.matmul(X, w) + b  \n",
    "\n",
    "\n",
    "error = Y_pred - Y                                               \n",
    "mse = tf.reduce_mean(tf.square(error), name = \"mse\")  \n",
    "\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())  # define a writer\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_all = optimizer.minimize(mse)\n",
    "\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)                  # define the summary scalars you want to track\n",
    "w_summary = tf.summary.scalar('w_1',  tf.reduce_mean(w))\n",
    "b_summary = tf.summary.scalar('b_1', tf.reduce_mean(b))\n",
    "merged = tf.summary.merge_all()                               # merge them for simplicity\n",
    "\n",
    "reportStep = 500\n",
    "tb_step = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        _, y_result, mse_result = sess.run([train_all,  Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "        if (epoch ) % reportStep == 0:\n",
    "            print('Epoch: ', epoch+1)\n",
    "            print('\\terror: ',mse_result)\n",
    "        if (epoch) % tb_step == 0:                                       # don't write at every step!\n",
    "            \n",
    "            summary_merged = sess.run(merged, feed_dict=feedDict)        # evaluate your merged scalars \n",
    "            file_writer.add_summary(summary_merged, epoch)               # write them to your directory\n",
    "            \n",
    "    w_calc, b_calc,mse_calc = \\\n",
    "        sess.run([w, b, mse],feed_dict=feedDict)\n",
    "    \n",
    "    print('\\nFinal values:')\n",
    "    print('\\tepochs: ',epoch+1)\n",
    "    print('\\terror: ',mse_result)\n",
    "    print('\\tPredicted Y: ', y_result)\n",
    "    print('\\tActual y: ', [[4],[6]])\n",
    "    print('\\tw: ',w_calc)\n",
    "    print('\\tb: ',b_calc)\n",
    "    \n",
    "    file_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Vars\">\n",
    "## Appendix B: A Better way to Manage Variables: Name Scopes & get_variable\n",
    "[Back to Table of Contents](#TOC)\n",
    "\n",
    "In deep networks, naming the individual nodes and layers can be rather intractable. A good way to manage this is through **name scopes**.  Also, using the tf.get_variable framework is a better way to define variables, as it allows for variable sharing across layers/components (with suitable option), etc:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final values:\n",
      "\tepochs:  5000\n",
      "\terror:  4.15357e-07\n",
      "\tPredicted Y:  [[ 3.99922442]\n",
      " [ 6.00047874]]\n",
      "\tActual y:  [[4], [6]]\n",
      "\tw:  [[ 1.00062644]]\n",
      "\tb:  [ 0.99734581]\n"
     ]
    }
   ],
   "source": [
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape  = (None,1), name = \"x\")  \n",
    "Y = tf.placeholder(tf.float32, shape=(None,1), name = \"y\")    \n",
    "\n",
    "#w = tf.Variable(tf.random_uniform(shape =(1,1)),name = \"w\")            \n",
    "#b = tf.Variable(tf.random_uniform(shape = (1,)),name = \"b\")  \n",
    "\n",
    "with tf.name_scope(\"LM\") as scope_1:\n",
    "    w = tf.get_variable(\"w\", (1,1), initializer=tf.random_normal_initializer())\n",
    "    b = tf.get_variable(\"b\", (1,), initializer=tf.random_normal_initializer())\n",
    "    \n",
    "    Y_pred = tf.matmul(X, w) + b    # a better notation would be:  Y_pred = tf.nn.bias_add(tf.matmul(X, w), b)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:                               # Adding the name scope\n",
    "    error = Y_pred - Y                                               \n",
    "    mse = tf.reduce_mean(tf.square(error), name = \"mse\")  \n",
    "\n",
    "\n",
    "file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_all = optimizer.minimize(mse)\n",
    "\n",
    "\n",
    "mse_summary = tf.summary.scalar('MSE', mse)\n",
    "w_summary = tf.summary.scalar('w_1',  tf.reduce_mean(w))\n",
    "b_summary = tf.summary.scalar('b_1', tf.reduce_mean(b))\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "reportStep = 500\n",
    "tb_step = 100\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "feedDict = {X:[[3], [5]], Y:[[4],[6]]} \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(5000):\n",
    "        _, y_result, mse_result = sess.run([train_all,  Y_pred, mse],feed_dict=feedDict)\n",
    "    \n",
    "        if (epoch) % tb_step == 0:\n",
    "            \n",
    "            summary_merged = sess.run(merged, feed_dict=feedDict)\n",
    "            file_writer.add_summary(summary_merged, epoch)\n",
    "            \n",
    "    w_calc, b_calc,mse_calc = \\\n",
    "        sess.run([w, b, mse],feed_dict=feedDict)\n",
    "    \n",
    "    print('\\nFinal values:')\n",
    "    print('\\tepochs: ',epoch+1)\n",
    "    print('\\terror: ',mse_result)\n",
    "    print('\\tPredicted Y: ', y_result)\n",
    "    print('\\tActual y: ', [[4],[6]])\n",
    "    print('\\tw: ',w_calc)\n",
    "    print('\\tb: ',b_calc)\n",
    "    \n",
    "    file_writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So? TensorFlow adds the prfix 'loss/' in front of the variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss/sub:0\", shape=(?, 1), dtype=float32)\n",
      "Tensor(\"loss/mse:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(error)\n",
    "print(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"LM/add:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(Y_pred)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
